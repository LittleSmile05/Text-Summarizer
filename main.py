def create_streamlit_app():
    """Streamlit veb t…ôtbiqini yaradƒ±r """
    st.set_page_config(page_title="Advanced Text Summarizer", page_icon="üìù", layout="wide")
    
    st.title("üìù Advanced Text Summarizer")
    st.markdown("""
    Bu t…ôtbiq m√ºxt…ôlif m…ôtn x√ºlas…ô texnikalarƒ±nƒ± g√∂st…ôrir:
    - **Tezlik …ôsaslƒ±**: Y√ºks…ôk tezlikli vacib s√∂zl…ôri olan c√ºml…ôl…ôri se√ßir
    - **TF-IDF**: Term Frequency-Inverse Document Frequency istifad…ô ed…ôr…ôk vacib c√ºml…ôl…ôri m√º…ôyy…ônl…ô≈üdirir
    - **TextRank**: Qraf …ôsaslƒ± reytinq alqoritmi t…ôtbiq edir (PageRank kimidir)
    - **Abstrakt**: BART transformer modeli il…ô yeni x√ºlas…ô yaradƒ±r
    
    Bu m…ônim ilk b√∂y√ºk layih…ômdir! Universitetd…ô son kursam :)
    """)
    
    # Tablar yaradƒ±rƒ±q - bunlarƒ± √∂z√ºm √∂yr…ôndim
    tab1, tab2, tab3 = st.tabs(["M…ôtn X√ºlas…ô", "√úsullarƒ± M√ºqayis…ô Et", "Haqqƒ±nda"])
    
    with tab1:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            text_input = st.text_area("X√ºlas…ô etm…ôk √º√ß√ºn m…ôtn daxil edin:", height=300)
            
            col1a, col1b, col1c = st.columns(3)
            
            with col1a:
                summarization_method = st.selectbox(
                    "X√ºlas…ô √ºsulunu se√ßin:",
                    ["Tezlik …ôsaslƒ±", "TF-IDF", "TextRank", "Abstrakt (BART)"]
                )
            
            with col1b:
                if summarization_method != "Abstrakt (BART)":
                    num_sentences = st.slider("X√ºlas…ôd…ôki c√ºml…ô sayƒ±:", 1, 10, 3)
                else:
                    max_length = st.slider("Maksimum x√ºlas…ô uzunluƒüu (token):", 50, 250, 150)
                    min_length = st.slider("Minimum x√ºlas…ô uzunluƒüu (token):", 10, 100, 50)
            
            with col1c:
                if st.button("X√ºlas…ô Et", type="primary"):
                    if text_input:
                        with st.spinner('X√ºlas…ô yaradƒ±lƒ±r... biraz g√∂zl…ôyin...'):
                            summarizer = Textimport nltk
import numpy as np
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx
from transformers import pipeline, BartTokenizer, BartForConditionalGeneration
import matplotlib.pyplot as plt
import streamlit as st
import time

# Lazƒ±m olan kitabxanalarƒ± y√ºkl…ôyirik
# M…ôn…ô t…ôl…ôb…ô dostlarƒ±m bunu √∂rg…ôtdil…ôr :D

try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/stopwords')
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('punkt')  # Ay bu c√ºml…ôl…ôri ayƒ±rmaq √º√ß√ºnd√ºr
    nltk.download('stopwords')  # Stop s√∂zl…ôr, m…ôs…ôl…ôn "v…ô", "bu", "o"... 
    nltk.download('wordnet')  # S√∂zl…ôr √º√ß√ºn l√ºƒü…ôt kimi

class TextSummarizer:
    """M√ºxt…ôlif √ºsullarla m…ôtn x√ºlas…ô ed…ôn sinif."""
    
    def __init__(self):
        # Vay, ingilis dilind…ô olan stop s√∂zl…ôr
        # Bunlarƒ± √ßƒ±xardacaƒüƒ±q √ß√ºnki x√ºlas…ôd…ô vacib deyill…ôr
        self.stop_words = set(stopwords.words('english'))
        self.lemmatizer = WordNetLemmatizer()
        
        # Bu BART modeli √ßox g√ºcl√ºd√ºr! M√º…ôllimim dedi ki, CNN-d…ô istifad…ô olunub
        # Amma y√ºkl…ôm…ôsi biraz vaxt apara bil…ôr, internetim yava≈üdƒ±r...
        self.tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')
        self.model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')
        self.summarizer = pipeline("summarization", model="facebook/bart-large-cnn")  # Bu asan yoldur!
    
    def preprocess_text(self, text):
        """M…ôtni t…ômizl…ôyir v…ô hazƒ±rlayƒ±r."""
        # H…ôr ≈üeyi ki√ßik h…ôrfl…ô yazƒ±rƒ±q - n…ô f…ôrqi var ki? :)
        text = text.lower()
        
        # X√ºsusi simvollar v…ô r…ôq…ôml…ôri silirik
        # Bir d…ôf…ô imtahanda bunu soru≈üdular, ona g√∂r…ô …ôzb…ôrl…ômi≈ü…ôm
        text = re.sub(r'[^\w\s]', '', text)
        text = re.sub(r'\d+', '', text)
        
        # Artƒ±q bo≈üluqlarƒ± silirik, √ß√ºnki √ßox bo≈üluq n…ôy…ô lazƒ±mdƒ±r?
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    def extract_sentences(self, text):
        """M…ôtnd…ôn c√ºml…ôl…ôri ayƒ±rƒ±r."""
        # Bu asan funksiya, sad…ôc…ô NLTK-nƒ±n sent_tokenize istifad…ô edirik
        # Qrupumuzda hamƒ± bel…ô edir, kitabdakƒ± kimi :D
        sentences = sent_tokenize(text)
        return sentences
    
    def extract_word_frequency(self, text):
        """Stop s√∂zl…ôri silir v…ô s√∂zl…ôrin tezliyini hesablayƒ±r."""
        # S√∂zl…ôri ayƒ±rƒ±rƒ±q - m…ônc…ô burda m…ônim kodumu m√º…ôllim b…ôy…ôn…ôc…ôk
        words = word_tokenize(text)
        
        # Stop s√∂zl…ôri silirik v…ô lemmatize edirik (…ôsas formaya salƒ±rƒ±q)
        # M…ôs…ôl…ôn, "running" -> "run" olur. √áox aƒüƒ±llƒ±dƒ±r!
        words = [self.lemmatizer.lemmatize(word) for word in words if word not in self.stop_words]
        
        # S√∂zl…ôrin tezliyini hesablayƒ±rƒ±q - bunu √∂z√ºm yazmƒ±≈üam :)
        # H…ômkarƒ±m dedi ki, Counter() istifad…ô etm…ôk olar amma m…ôn bel…ô ba≈üa d√º≈ü√ºr…ôm
        word_freq = {}
        for word in words:
            if word in word_freq:
                word_freq[word] += 1
            else:
                word_freq[word] = 1
        return word_freq
    
    def score_sentences(self, sentences, word_freq):
        """C√ºml…ôl…ôri s√∂zl…ôrin tezliyin…ô g√∂r…ô qiym…ôtl…ôndirir."""
        # H…ôr c√ºml…ônin balƒ±nƒ± hesablayƒ±rƒ±q
        # ƒ∞lk d…ôf…ô bunu g√∂r…ônd…ô √ßox √ß…ôtin idi, amma anladƒ±m ki …ôslind…ô sad…ôdir
        sentence_scores = {}
        for i, sentence in enumerate(sentences):
            # C√ºml…ôd…ôki h…ôr s√∂z…ô baxƒ±rƒ±q
            for word in word_tokenize(sentence.lower()):
                if word in word_freq:  # ∆èg…ôr bu s√∂z vacibdirs…ô
                    if i in sentence_scores:
                        sentence_scores[i] += word_freq[word]  # Balƒ± artƒ±rƒ±rƒ±q
                    else:
                        sentence_scores[i] = word_freq[word]  # ƒ∞lk bal
        return sentence_scores
    
    def frequency_based_summary(self, text, num_sentences=3):
        """S√∂z tezliyi …ôsasƒ±nda x√ºlas…ô yaradƒ±r."""
        # ƒ∞lk √∂nc…ô m…ôtni hazƒ±rlayƒ±rƒ±q
        processed_text = self.preprocess_text(text)
        # Sonra c√ºml…ôl…ôri ayƒ±rƒ±rƒ±q (orijinal m…ôtnd…ôn, √ß√ºnki daha yax≈üƒ± olur)
        sentences = self.extract_sentences(text)
        # S√∂zl…ôrin tezliyini hesablayƒ±rƒ±q
        word_freq = self.extract_word_frequency(processed_text)
        # C√ºml…ôl…ôri qiym…ôtl…ôndiririk
        sentence_scores = self.score_sentences(sentences, word_freq)
        
        # ∆èn y√ºks…ôk ballƒ± n c√ºml…ôni se√ßirik
        # sorted() funksiyasƒ±nƒ± d…ôrsd…ô √∂yr…ôndik, √ßox faydalƒ±dƒ±r!
        top_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)[:num_sentences]
        top_sentences = sorted(top_sentences, key=lambda x: x[0])  # M…ôtnd…ôki sƒ±raya g√∂r…ô d√ºz√ºr√ºk
        
        # X√ºlas…ôni yaradƒ±rƒ±q - c√ºml…ôl…ôri sad…ôc…ô birl…ô≈üdiririk
        summary = ' '.join([sentences[i] for i, _ in top_sentences])
        return summary
    
    def tfidf_based_summary(self, text, num_sentences=3):
        """TF-IDF istifad…ô ed…ôr…ôk x√ºlas…ô yaradƒ±r. Bu daha elmidir :)"""
        # C√ºml…ôl…ôri ayƒ±rƒ±rƒ±q
        sentences = self.extract_sentences(text)
        
        # TF-IDF vektorizatoru yaradƒ±rƒ±q
        # Bunu d…ôrslikd…ôn kopyaladƒ±m, d√ºz√º tam ba≈üa d√º≈üm√ºr…ôm, amma i≈ül…ôyir!
        vectorizer = TfidfVectorizer(stop_words='english')
        tfidf_matrix = vectorizer.fit_transform(sentences)
        
        # C√ºml…ôl…ôrin balƒ±nƒ± TF-IDF d…ôy…ôrl…ôrin…ô g√∂r…ô hesablayƒ±rƒ±q
        # Burada bir az √ß…ôtinlik √ß…ôkdim amma internetd…ôn baxƒ±b anladƒ±m
        sentence_scores = [sum(tfidf_matrix[i].toarray()[0]) for i in range(len(sentences))]
        
        # ∆èn y√ºks…ôk ballƒ± n c√ºml…ôni se√ßirik
        # argsort() - indeksl…ôri sƒ±ralayƒ±r, √ßox maraqlƒ± funksiyadƒ±r!
        top_indices = np.argsort(sentence_scores)[-num_sentences:]
        top_indices = sorted(top_indices)  # M…ôtnd…ôki sƒ±raya g√∂r…ô d√ºz√ºr√ºk
        
        # X√ºlas…ôni yaradƒ±rƒ±q
        summary = ' '.join([sentences[i] for i in top_indices])
        return summary
    
    def textrank_summary(self, text, num_sentences=3):
        """TextRank alqoritmi il…ô x√ºlas…ô yaradƒ±r - bu Google PageRank kimidir!"""
        # C√ºml…ôl…ôri ayƒ±rƒ±rƒ±q
        sentences = self.extract_sentences(text)
        
        # Ox≈üarlƒ±q matrisini yaradƒ±rƒ±q
        # Allah bilir bu nec…ô i≈ül…ôyir amma m√º…ôllim dedi yax≈üƒ± √ºsuldur :D
        vectorizer = TfidfVectorizer(stop_words='english')
        tfidf_matrix = vectorizer.fit_transform(sentences)
        similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)
        
        # TextRank alqoritmini NetworkX il…ô t…ôtbiq edirik
        # Qraflardakƒ± tap≈üƒ±rƒ±ƒüƒ± ed…ônd…ô bunu √∂yr…ôndim, √ßox mentalitet ≈üeydir
        nx_graph = nx.from_numpy_array(similarity_matrix)
        scores = nx.pagerank(nx_graph)
        
        # ∆èn y√ºks…ôk ballƒ± n c√ºml…ôni se√ßirik
        ranked_sentences = sorted(((scores[i], i) for i in range(len(sentences))), reverse=True)
        top_sentences = sorted(ranked_sentences[:num_sentences], key=lambda x: x[1])
        
        # X√ºlas…ôni yaradƒ±rƒ±q
        summary = ' '.join([sentences[i] for _, i in top_sentences])
        return summary
    
    def abstractive_summary(self, text, max_length=150, min_length=50):
        """BART modeli il…ô abstrakt x√ºlas…ô yaradƒ±r - bu tamamil…ô yeni c√ºml…ôl…ôr yazƒ±r!"""
        # M…ôtni modelin anlaya bil…ôc…ôyi ≈ü…ôkild…ô k…ôsirik
        # Tokens are weird stuff - r…ôq…ôm kimi g√∂r√ºn√ºr amma s√∂z dem…ôkdir?
        max_tokens = 1024
        inputs = self.tokenizer.encode(text, return_tensors="pt", max_length=max_tokens, truncation=True)
        
        # X√ºlas…ôni generasiya edirik
        # Bu ba≈üaƒürƒ±sƒ± parametrl…ôri proyektd…ôn kopyaladƒ±m, t…ôsad√ºf…ôn silm…ô :D
        summary_ids = self.model.generate(
            inputs, 
            max_length=max_length, 
            min_length=min_length, 
            length_penalty=2.0,  # N…ô √º√ß√ºn 2.0? ƒ∞nternetd…ô bel…ô yazƒ±lƒ±b...
            num_beams=4,  # M…ônc…ô bu "nur" kimi bir ≈üeydir
            early_stopping=True
        )
        
        # X√ºlas…ôni decode edirik (y…ôni oxunaqlƒ± m…ôtn…ô √ßeviririk)
        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        return summary
    
    def pipeline_summary(self, text, max_length=150, min_length=50):
        """Pipeline API il…ô x√ºlas…ô, daha asandƒ±r."""
        # M…ôtni k…ôsirik lazƒ±m g…ôl…ôrs…ô
        # 100000 √ßox b√∂y√ºk r…ôq…ômdir, he√ß kim bel…ô uzun m…ôtn yazmaz!
        if len(text) > 100000:
            text = text[:100000]
            
        # Pipeline-da summarize edirik - bir s…ôtird…ô! M√∂ht…ô≈ü…ôm!
        result = self.summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)
        return result[0]['summary_text']
    
    def evaluate_summary(self, original_text, summary):
        """X√ºlas…ônin keyfiyy…ôtini qiym…ôtl…ôndirir - …ôslind…ô m…ônim fikrimc…ô subyektivdir!"""
        # Sƒ±xƒ±lma nisb…ôtini hesablayƒ±rƒ±q - y…ôni n…ô q…ôd…ôr ki√ßiltdik
        original_words = len(word_tokenize(original_text))
        summary_words = len(word_tokenize(summary))
        compression = (1 - summary_words / original_words) * 100  # Faiz…ô √ßeviririk
        
        # L√ºƒü…ôt √∂rt√ºy√ºn√º hesablayƒ±rƒ±q - bu m…ônim …ôlav…ômdir
        # Y…ôni x√ºlas…ôd…ô orijinal m…ôtnd…ôn n…ô q…ôd…ôr s√∂z var?
        original_vocab = set(word_tokenize(self.preprocess_text(original_text)))
        summary_vocab = set(word_tokenize(self.preprocess_text(summary)))
        overlap = len(original_vocab.intersection(summary_vocab)) / len(original_vocab) * 100
        
        # B√ºt√ºn m…ôlumatlarƒ± qaytarƒ±rƒ±q
        # Dictionary yaratmaqda dictionary comprehension daha yax≈üƒ± olardƒ± amma bel…ô ba≈üa d√º≈ü√ºr…ôm
        return {
            "compression_ratio": compression,  # N…ô q…ôd…ôr sƒ±xƒ±lƒ±b
            "vocabulary_overlap": overlap,     # N…ô q…ôd…ôr s√∂z saxlanƒ±lƒ±b
            "original_length": original_words, # Orijinal uzunluq
            "summary_length": summary_words    # X√ºlas…ô uzunluƒüu
        }
    
    def visualize_summary_comparison(self, text, methods=['frequency', 'tfidf', 'textrank', 'abstractive']):
        """M√ºxt…ôlif x√ºlas…ô √ºsullarƒ±nƒ± vizual m√ºqayis…ô edir - bunu t…ôqdimatda t…ôqdim ed…ô bil…ôr…ôm!"""
        summaries = {}
        evaluation = {}
        
        # M√ºxt…ôlif √ºsullarla x√ºlas…ôl…ôr yaradƒ±rƒ±q
        # Loop n…ôdir? D√∂ng√º, t…ôkrar dem…ôk ist…ôyir…ôm! :)
        for method in methods:
            if method == 'frequency':
                summaries[method] = self.frequency_based_summary(text)
            elif method == 'tfidf':
                summaries[method] = self.tfidf_based_summary(text)
            elif method == 'textrank':
                summaries[method] = self.textrank_summary(text)
            elif method == 'abstractive':
                summaries[method] = self.abstractive_summary(text)
            
            # H…ôr x√ºlas…ôni qiym…ôtl…ôndiririk
            evaluation[method] = self.evaluate_summary(text, summaries[method])
        
        # Vizualla≈üdƒ±rmalarƒ± yaradƒ±rƒ±q
        # Bu qrafikl…ôri d…ô m√º…ôllimim √∂yr…ôtdi, r…ôngar…ông olur!
        metrics = ['compression_ratio', 'vocabulary_overlap', 'summary_length']
        method_names = list(evaluation.keys())
        
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        for i, metric in enumerate(metrics):
            values = [evaluation[method][metric] for method in method_names]
            axes[i].bar(method_names, values)  # Bu bar qrafikdir, sevimli qrafikimdir
            axes[i].set_title(f'{metric.replace("_", " ").title()}')  # Ba≈ülƒ±ƒüƒ± g√∂z…ôll…ô≈üdiririk
            axes[i].set_ylabel('Value')  # Y oxunun adƒ±
            if metric == 'compression_ratio' or metric == 'vocabulary_overlap':
                axes[i].set_ylabel('Faiz (%)')  # Faizdirs…ô bel…ô yazƒ±rƒ±q
            
        plt.tight_layout()  # Qrafikl…ôri d√ºzg√ºn yerl…ô≈üdirir
        return fig, summaries, evaluation


def create_streamlit_app():
    """Create a Streamlit web application for the text summarizer."""
    st.set_page_config(page_title="Advanced Text Summarizer", page_icon="üìù", layout="wide")
    
    st.title("üìù Advanced Text Summarizer")
    st.markdown("""
    This application demonstrates multiple text summarization techniques:
    - **Frequency-based**: Extracts sentences with high-frequency important words
    - **TF-IDF**: Uses Term Frequency-Inverse Document Frequency to identify important sentences
    - **TextRank**: Applies a graph-based ranking algorithm (similar to PageRank)
    - **Abstractive**: Generates a new summary using BART transformer model
    """)
    
    # Create tabs
    tab1, tab2, tab3 = st.tabs(["Text Summarization", "Compare Methods", "About"])
    
    with tab1:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            text_input = st.text_area("Enter the text to summarize:", height=300)
            
            col1a, col1b, col1c = st.columns(3)
            
            with col1a:
                summarization_method = st.selectbox(
                    "Select summarization method:",
                    ["Frequency-based", "TF-IDF", "TextRank", "Abstractive (BART)"]
                )
            
            with col1b:
                if summarization_method != "Abstractive (BART)":
                    num_sentences = st.slider("Number of sentences in summary:", 1, 10, 3)
                else:
                    max_length = st.slider("Maximum summary length (tokens):", 50, 250, 150)
                    min_length = st.slider("Minimum summary length (tokens):", 10, 100, 50)
            
            with col1c:
                if st.button("Generate Summary", type="primary"):
                    if text_input:
                        with st.spinner('Generating summary...'):
                            summarizer = TextSummarizer()
                            
                            # Apply selected summarization method
                            if summarization_method == "Frequency-based":
                                summary = summarizer.frequency_based_summary(text_input, num_sentences)
                            elif summarization_method == "TF-IDF":
                                summary = summarizer.tfidf_based_summary(text_input, num_sentences)
                            elif summarization_method == "TextRank":
                                summary = summarizer.textrank_summary(text_input, num_sentences)
                            else:  # Abstractive
                                summary = summarizer.abstractive_summary(text_input, max_length, min_length)
                                
                            # Evaluate summary
                            eval_metrics = summarizer.evaluate_summary(text_input, summary)
                            
                            # Store results in session state
                            st.session_state.summary = summary
                            st.session_state.eval_metrics = eval_metrics
                            st.session_state.original_text = text_input
                    else:
                        st.error("Please enter some text to summarize.")
        
        with col2:
            st.subheader("Summary Output")
            
            if 'summary' in st.session_state:
                st.markdown("### Generated Summary")
                st.write(st.session_state.summary)
                
                st.markdown("### Summary Statistics")
                metrics = st.session_state.eval_metrics
                st.markdown(f"**Original Length:** {metrics['original_length']} words")
                st.markdown(f"**Summary Length:** {metrics['summary_length']} words")
                st.markdown(f"**Compression Ratio:** {metrics['compression_ratio']:.2f}%")
                st.markdown(f"**Vocabulary Overlap:** {metrics['vocabulary_overlap']:.2f}%")
                
                # Option to download summary
                st.download_button(
                    label="Download Summary",
                    data=st.session_state.summary,
                    file_name="text_summary.txt",
                    mime="text/plain"
                )
            else:
                st.info("Your summary will appear here.")
    
    with tab2:
        st.subheader("Compare Summarization Methods")
        
        compare_text = st.text_area("Enter text to compare different summarization methods:", height=200)
        
        if st.button("Compare Methods"):
            if compare_text:
                with st.spinner('Comparing methods...'):
                    summarizer = TextSummarizer()
                    
                    # Get comparison results
                    fig, summaries, evaluations = summarizer.visualize_summary_comparison(compare_text)
                    
                    # Display chart
                    st.pyplot(fig)
                    
                    # Display summaries
                    st.subheader("Summaries by Method")
                    for method, summary in summaries.items():
                        with st.expander(f"{method.title()} Summary"):
                            st.write(summary)
                            metrics = evaluations[method]
                            st.markdown(f"**Compression:** {metrics['compression_ratio']:.2f}% | **Overlap:** {metrics['vocabulary_overlap']:.2f}% | **Length:** {metrics['summary_length']} words")
            else:
                st.error("Please enter some text to compare methods.")
    
    with tab3:
        st.subheader("About This Project")
        st.markdown("""
        ### Advanced Text Summarization Project
        
        This project implements multiple text summarization techniques using a combination of traditional NLP approaches and modern transformer-based models.
        
        #### Features:
        - Multiple summarization algorithms (frequency-based, TF-IDF, TextRank, and abstractive BART)
        - Summary evaluation metrics
        - Visual comparison of different methods
        - User-friendly web interface
        
        #### Technologies Used:
        - Python
        - NLTK for natural language processing
        - scikit-learn for TF-IDF and cosine similarity
        - NetworkX for graph-based algorithms
        - Hugging Face Transformers for BART model
        - Streamlit for the web interface
        - Matplotlib for visualization
        
        #### Potential Applications:
        - Summarizing news articles
        - Creating abstracts for research papers
        - Condensing long documents or reports
        - Generating summaries for content curation
        
        #### Future Improvements:
        - Multi-language support
        - More comprehensive evaluation metrics (ROUGE, BLEU)
        - Fine-tuning the transformer model for specific domains
        - Additional summarization algorithms
        """)


if __name__ == "__main__":
    create_streamlit_app()
